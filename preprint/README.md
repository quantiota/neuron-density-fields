#  Preprint Overview

## Abstract
Introduce Riemannian Neural Fields (RNF) as an entropy-driven system operating in curved geometric spaces using Simplex-generated substrates.

## 1. Introduction
Define the limitations of Euclidean parameter spaces in capturing the non-linear structures of continuous domains.

## 2. Theoretical Substrate
Explain the metric tensor $\( g_{ij}(r) \)$ and its dependence on density gradients $\( \nabla \rho \)$.

## 3. Noise Rationale
Use the comparison table to show why Perlin noise and other alternatives fail due to directional artifacts or exponential computational costs in 4D and 5D.

## 4. Simplex Scaling
Detail how *n*-simplex geometry scales linearly with dimension $(\( n + 1 \)$ vertices), enabling $\( O(n^2) \)$ computational complexity.

## 5. Biological Substrate
Present the 3D, 4D, and 5D visualizations showing biologically plausible neuron density variations (30k–180k neurons/mm³).

## 6. Statistical Analysis
Discuss convergence toward the mean via the central limit theorem as dimensionality increases.

## 7. Conclusion
Summarize how this substrate enables automated architecture discovery and geodesic learning.
